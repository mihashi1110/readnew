# 4. probabilistic Learning using Naive Bayes

## Introduction
Classifiers based on Bayesian methods utilize training data to calculate an observed probability of each outcome based on the evidence provided by feature values. 
Typically, Bayesian classifiers are best applied to problems in which the information from numerous attributes should be considered simultaneously in order to estimate the overall probability of an outcome
For example, a 70 percent chance of rain implies that in 7 out of the 10 past cases with similar conditions.

- Basic concepts 
-- we use notation in the form P(A), which signifies the probability of event A.
If we assumed that 
--- E1 – train cancelled
--- E2 – train ten minutes or more late 
--- E3 – train less than ten minutes late 
--- E4 – train on time or early. 
 we might have P (E1) = 0.05, P(E2) = 0.1, P (E3) = 0.15, P(E4) = 0.7  and P(E1) + P(E2) + P(E3) + P(E4) = 1 

- Joint probability 
The relationships between dependent events can be described using Bayes' theorem, as shown in the following formula
$$p\left(A\left|B\right.\right)=\frac{p\left(A\cap B\right)}{p\left(B\right)}$$ 
Rearranging this formula once more with the knowledge to obtain the **posterior probability** based on the prior probability $p(A)$ and likelihood $p\left(B\left|A\right.\right)$.
$$p\left(A\left|B\right.\right)=\frac{p\left(A\cap B\right)}{p\left(B\right)}=\frac{p\left(B\left|A\right.\right)p\left(A\right)}{p(B)}$$

-- Suppose that our training set consists of 20 instances, each recording the value of four attributes as well as the classification. We will use classifications: **cancelled**, **very late**, **late** and **on time** to correspond to the events E1, E2, E3 and E4 described previously.
![](https://i.imgur.com/Vycdx2r.png)
we want to calculate the probability of P (class = on time | day = weekday and season = winter and wind = high and rain = heavy) and do similarly for the other three possible classifications (cancelled, very late, late ) 
![](https://i.imgur.com/OmDtdTi.png)

We could start by using conditional probabilities based on a **single** attribute. 
--- P (class = on time | season = winter) = 2/6 = 0.33 
--- P (class = late | season = winter) = 1/6 = 0.17, 
--- P (class = very late | season = winter) = 3/6 = 0.5 
--- P (class = cancelled | season = winter) = 0/6 = 0.

If we have many information such as $E_1, E_2, E_3, E_4$, then 
$$p\left(\textrm{on time}\left|E_{1}\cap E_{2}\cap E_{3}\cap E_{4}\right.\right)=\frac{p\left(E_{1}\cap E_{2}\cap E_{3}\cap E_{4}\left|\textrm{on time}\right.\right)p\left(\textrm{on time}\right)}{p\left(E_{1}\cap E_{2}\cap E_{3}\cap E_{4}\right)}$$

This formula is computationally difficult to solve. Specifically, it assumes class-conditional independence, which means that events are Independent so long as they are conditioned on the same class value 
$$
p\left(\textrm{on time}\left|E_{1}\cap E_{2}\cap E_{3}\cap E_{4}\right.\right)\propto\left[\sum_{i=1}^{4}p\left(E_{i}\left|\textrm{on time}\right.\right)\right]p\left(\textrm{on time}\right)
$$

The Naive Bayes algorithm gives us a way of combining the **prior probability** and **conditional probabilities** in a single formula to calculate the probability of each of the possible classifications in turn.
![](https://i.imgur.com/JKTPwnQ.png)

Combines the prior probability of $c_i$ with the values of the $n$ possible conditional probabilities involving a test on the value of a single attribute.

$$
p\left(c_{i}\right)\prod_{j=1}^{n}p\left(a_{j}=v_{j}\left|class=c_{i}\right.\right)
$$

For our story, P (class = on time | day = weekday and season = winter and wind = high and rain = heavy) 
--- class = on time 0.70 × 0.64 × 0.14 × 0.29 × 0.07 = 0.0013 
--- class = late 0.10 × 0.50 × 1.00 × 0.50 × 0.50 = 0.0125 
--- class = very late 0.15 × 1.00 × 0.67 × 0.33 × 0.67 = 0.0222 
--- class = cancelled 0.05 × 0.00 × 0.00 × 1.00 × 1.00 = 0.0000 

## The Laplace estimator 
If there is a ZERO conditional probability, i.e. $p\left(E_{i}\left|\textrm{on time}\right.\right)=0$, then the above is over. A solution to this problem involves using something called the Laplace estimator, which is named after the French mathematician Pierre-Simon Laplace. A solution to this problem involves using something called the Laplace estimator, which is named after the French mathematician Pierre-Simon Laplace. Using a Laplace value of **1**, we add one to each numerator in the likelihood function. 

## R Case - Dibetes with Naive Bayes Algorithm 
### Step 1 – collecting data 
The dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. There are 767 examples of cancer biopsies, each with 9 features. One feature is an identification number, another is the cancer diagnosis, and 30 are numeric-valued laboratory measurements. 
```{r}
getwd()  # put the usedcars.csv into working location
data <-read.csv("C:/Users/.../diabetes.csv", header=T, sep=",")
str(data)
```

- Pregnancies: Number of pregnancies so far
- Glucose: Plasma glucose concentration
- BloodPressure: Diastolic blood pressure (mm Hg)
- SkinThickness: Triceps skin fold thickness (mm)
- Insulin: 2-Hour serum insulin (mu U/ml)
- BMI: Body mass index (weight in kg/(height in m)^2)
- DiabetesPedigreeFunction: Diabetes pedigree function
- Age: Age (years)
- The response variable or the output variable is: Class variable (0 or 1)

### Step 2 exploring and preparing the data 
importing the CSV data file, check the missing value.
```{r}
diab <- data
colSums(is.na(diab)) # missing data check
```

when we look at the prop.table() output, we notice that the values have been labeled Health and Diabete with 65.1 percent and 34.9 percent of the masses, respectively.
```{r}
round(prop.table(table(diab$Outcome)) * 100, digits = 1) 
```

### 2.1 Transformation – normalizing numeric data 
we can use lapply() to apply normalize() to each feature in the data frame. 

```{r}
summary(diab)
normalize <- function(x) { 
            return ((x - min(x)) / (max(x) - min(x))) 
             }
diab_n <- as.data.frame(lapply(diab[1:9], normalize)) 

summary(diab_n)
```
### 2.2 Split the diab_n data frame into diab_train and diab_test

```{r}
set.seed(100)
index <-  sample(nrow(diab_n), nrow(diab_n)*0.7)
diab_train <- diab_n[index,]
diab_test <- diab_n[-index,]
table(diab$Outcome)

dim(diab_train)
dim(diab_test)
```

### Step 3 – Training A Model on the Data 
To classify our test instances, we will use the naiveBayes implementation from the e1071 package, which provides a set of basic R functions for classification. 
```{r}
install.packages("e1071") 
library(e1071) # for naive bayes
model <- naiveBayes(as.factor(Outcome) ~., data = diab_train, laplace = 1) 

trainPred <- predict(model, newdata = diab_train)

mean(trainPred != diab_train$Outcome)
table(diab_train$Outcome,trainPred)
```
### Step 4 – Evaluating Model Pperformance 
```{r}
install.packages("caret")
library(caret)
confusionMatrix(as.factor(diab_train$Outcome), trainPred) 

testPred <- predict(model, newdata = diab_test)

mean(testPred != diab_test$Outcome)
table(diab_test$Outcome,testPred)
confusionMatrix(as.factor(diab_test$Outcome), testPred)
```
The top-left cell indicates the true negative results. The bottom-right cell indicates the true positive results. lower-left cell are false negative results; false positive results. 

### Confusion Matrix
it is a performance measurement for the classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values.
![](https://i.imgur.com/hCnQmBP.png)

It is extremely useful for measuring Recall, Precision, Specificity, Accuracy and most importantly AUC-ROC Curve.

- True Positive
- True Negative
- False Positive: (Type 1 Error)
- False Negative: (Type 2 Error)

- Recall
$$
Recall=\frac{TP}{TP+TN}
$$
- Precision
$$
Precision=\frac{TP}{TP+FP}
$$
- Accuracy: predicted correctly
$$
Accuracy=\frac{TP+TN}{TP+FP+FN+TN}
$$
- F-measure
$$
F-meature=\frac{2*Recall*Precision}{Recall+Precision}
$$




