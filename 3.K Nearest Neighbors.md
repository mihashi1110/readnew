# 3. Lazy Learning ‚Äì Classification Using K Nearest Neighbors 


## Introduction
"Birds of a feather flock together". Data science uses this principle to classify data by placing it in the same category as **similar** or "**k nearest**" neighbors.
**K nearest neighbors** is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions).

You will learn: 
- The key concepts that define nearest neighbor classifiers, and why they are considered "lazy" learners 
- Methods to measure the similarity of two examples using distance 
- To apply a popular nearest neighbor classifier called k-NN 

# Algorithm
Nearest neighbor classifiers are well-suited for **classification** tasks. The nearest neighbors approach to classification is exemplified by the **k-nearest neighbors algorithm** (k-NN). The letter $k$ is a variable term implying that any number of nearest neighbors could be used. In particular, the "nearest" in similarity. The k-NN algorithm treats the features as coordinates in a multidimensional feature space
![](https://i.imgur.com/kqhvEEe.png)

## Measuring similarity with distance 
If we use the k-NN algorithm with k = 3 instead, it performs a vote among the three nearest neighbors: orange, grape, and nuts. Since the majority class among these neighbors is fruit (two of the three votes), the tomato again is classified as a fruit.

Traditionally, the k-NN algorithm uses **Euclidean distance**, which is the distance one would measure if it were possible to use a ruler to connect two points.

A case is then classified by a **majority vote** of its neighbors, with the case being assigned to the class most common amongst its K nearest neighbors measured by a **distance function**. 

Distance Function

Euclidean $\left(\sum_{i=1}^{k}(x_{i}-y_{i})^{2}\right)^{1/2}$

Manhattan $\sum_{i=1}^{k}|x_{i}-y_{i}|$

Minkowski $\left(\sum_{i=1}^{k}\left(|x_{i}-y_{i}|\right)^{q}\right)^{1/q}$


## Appropriate k 
The balance between over fitting and under fitting the training data is a problem known as **bias-variance tradeoff**. Choosing a large k reduces the impact or variance caused by noisy data, but can bias the learner 
![](https://i.imgur.com/ln3iZPo.png)

- One common practice is to begin with k equal to the **square root of the number of training examples**. Choosing the optimal value for K is best done by first inspecting the data. 
- In general, a large K value is more precise as it reduces the overall noise but there is no guarantee. 
- Cross-validation is another way to retrospectively determine a good K value by using an independent dataset to validate the K value. 
- Historically, the optimal K for most datasets has been between 3-10. 

## Preparing data for use with k-NN 
Before algorithm for nearest neighbor classification, Features are typically transformed to a standard range prior to applying the k-NN algorithm. 

The traditional method of rescaling features for k-NN is **min-max normalization**. This process transforms a feature such that all of its values fall in a range between 0 and 1.
$$
x=\frac{x-x_{min}}{x_{max}-x_{min}}
$$

Another common transformation is called z-score standardization.
$$
x=\frac{x-mean(x)}{std\left(x\right)}
$$

- The Euclidean distance formula is not defined for **nominal data**. It should also be noted that all three distance measures are only valid for **continuous variables**. 
- In the instance of **categorical variables** the **Hamming distance** must be used. It also brings up the issue of standardization of the numerical variables between 0 and 1 when there is a mixture of numerical and categorical variables in the dataset.

Hamming Distance 
$$
D_{H}=\sum_{i=1}^{k}|x_{i}-y_{i}|
$$
  
$$\begin{array}{c}
x=y\Rightarrow D=0\\
x\neq y\Rightarrow D=1
\end{array}
$$

| X    | y      | Distance |
| ---- | ------ | -------- |
| Male | Male   |    0     |
| Male | Female |    1     |

Typical solution utilizes dummy coding, where a value of 1 indicates one category, and 0, the other. 

$$
male=\begin{cases}
\begin{array}{l}
1,\\
0,
\end{array} & \begin{array}{l}
\textrm{if x = male}\\
otherwise
\end{array}\end{cases}
$$

An n-category nominal feature can be dummy coded by creating the binary indicator variables for (n - 1) levels of the feature. 
The dummy coding for a three-category temperature variable (for example, hot, medium, or cold) could be set up as (3 - 1) = 2 features 
$$
hot=\begin{cases}
\begin{array}{l}
1,\\
0,
\end{array} & \begin{array}{l}
\textrm{if x = hot}\\
otherwise
\end{array}\end{cases}
$$
$$
ùëöùëíùëëùëñùë¢ùëö=\begin{cases}
\begin{array}{l}
1,\\
0,
\end{array} & \begin{array}{l}
\textrm{if x = ùëöùëíùëëùëñùë¢ùëö}\\
otherwise
\end{array}\end{cases}
$$

A convenient aspect of dummy coding is that the distance between dummy coded features is always one or zero, and thus, the values fall on the same scale as min-max normalized numeric data.

## Example:
Let's consider the following data concerning credit default. Age and Loan are two **numerical variables** (predictors) and Default is the target. We can now use the training set to classify an unknown case **(Age=48 and Loan=$142,000)** using Euclidean distance. If K=1 then the nearest neighbor is the last case in the training set with Default=Y.


$D=\sqrt{\left(48-33\right)^{2}+\left(142000-150000\right)^{2}}=8000.01\rightarrow Y$


![](https://i.imgur.com/CLKgfXN.png)


With $K=3$, there are two Default=Y and one Default=N out of three closest neighbors. The prediction for the unknown case is again Default=Y.

### Standardized Distance
One major drawback in calculating distance measures directly from the training set is in the case where variables have different measurement scales or there is a mixture of numerical and categorical variables. For example, if one variable is based on annual income in dollars, and the other is based on age in years then income will have a much higher influence on the distance calculated. One solution is to standardize the training set as shown below.

![](https://i.imgur.com/oo92jDa.png)

Using the standardized distance on the same training set, the unknown case returned **a different neighbo**r which is not a good sign of robustness.


### Why is the k-NN algorithm lazy? 
A lazy learner is not really learning anything. Due to the heavy reliance on the training instances rather than an abstracted model, lazy learning is also known as instance-based learning or rote learning. As **instance-based learners** do not build a model, the method is said to be in a class of non-parametric learning methods‚Äîno parameters are learned about the data.

## R Case - Diagnosing Breast Cancer with k-NN Algorithm 
### Step 1 ‚Äì collecting data 
we utilize the Wisconsin Breast Cancer Diagnostic dataset from the UCI Machine Learning Repository at http://archive.ics.uci.edu/ml. There are 569 examples of cancer biopsies, each with 32 features. One feature is an identification number, another is the cancer diagnosis, and 30 are numeric-valued laboratory measurements. 
```{r}
getwd()  # put the usedcars.csv into working location
wbcd <- data<-read.csv("C:/Users/.../bcwd.csv", header=T, sep=",")
str(wbcd)
```

### Step 2 exploring and preparing the data 
importing the CSV data file, 
Using the str(wbcd) command 
Let's drop the id feature altogether
```{r}
wbcd <- wbcd[-1] 
table(wbcd$diagnosis)  # table() output indicates that 357 masses are benign while 212 are malignant
```
classifiers require that the target feature is coded as a factor, so we will need to recode the diagnosis variable.
```{r}
wbcd$diagnosis<- factor(wbcd$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant")) 
```

when we look at the prop.table() output, we notice that the values have been labeled Benign and Malignant with 62.7 percent and 37.3 percent of the masses, respectively.
```{r}
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1) 
```
The remaining 30 features are all numeric, and as expected, they consist of three different measurements of ten characteristics 

### 2.1 Transformation ‚Äì normalizing numeric data 
we can use lapply() to apply normalize() to each feature in the data frame. 

```{r}
normalize <- function(x) { 
            return ((x - min(x)) / (max(x) - min(x))) 
             }
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize)) 
summary(wbcd_n$area_mean) # Apply a Function over a List or Vector
```
### 2.2 Split the wbcd_n data frame into wbcd_train and wbcd_test

```{r}
wbcd_train <- wbcd_n[1:469, ] #only feature
wbcd_test <- wbcd_n[470:569, ] 
wbcd_train_labels <- wbcd[1:469, 1]  # only decision and changed into factor
wbcd_test_labels <- as.factor(wbcd[470:569, 1]) # 08/19 amend !!!!!

```

### Step 3 ‚Äì Training A Model on the Data 
To classify our test instances, we will use a k-NN implementation from the class package, which provides a set of basic R functions for classification. 
```{r}
install.packages("class") 
library(class)
```
As our training data includes 469 instances, we might try k = 21, an odd number roughly equal to the square root of 469.

```{r}
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21) # Knn(train, test, class, k) and the result is the factor
mean(wbcd_test_pred != wbcd_test_labels) # both needs the factor formate
```
### Step 4 ‚Äì Evaluating Model Pperformance 
```{r}
install.packages("caret")
library(caret)
confusionMatrix(wbcd_test_labels, wbcd_test_pred) 
```
The top-left cell indicates the true negative results. These 77 of 100 values are cases where the mass was benign and the k-NN algorithm correctly identified it as such. The bottom-right cell indicates the true positive results. lower-left cell are false negative results; false positive results. A total of 2 out of 100, or 2 percent of masses were incorrectly classified by the k-NN approach. While 98 percent accuracy seems impressive for a few lines of R code.

### Step 5 ‚Äì Improving Model Performance 
Testing alternative values of k. Using the normalized training and test datasets, the same 100 records were classified using several different k values. The number of false negatives and false positives are shown for each iteration.

```{r}
wbcd_test_pred = NULL
error.rate = NULL
for(i in 1:32){
    set.seed(101)
    wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = i) 
    error.rate[i] = mean(wbcd_test_pred != wbcd_test_labels)
}
print(error.rate)

library(ggplot2)
k.values <- 1:32
error.df <- data.frame(error.rate,k.values)
error.df
ggplot(error.df,aes(x=k.values,y=error.rate)) + geom_point()+ geom_line(lty="dotted",color='red')
```
```{r}
# split into train and test
library(caTools)
set.seed(101)
sample <- sample.split(final.data$Species, SplitRatio = .70)
train <- subset(final.data, sample == TRUE)
test <- subset(final.data, sample == FALSE)

# normalizing numeric data
stand.features <- scale(iris[1:4])
```
